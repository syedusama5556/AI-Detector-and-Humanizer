{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c657dded",
      "metadata": {
        "id": "c657dded"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd08accd",
      "metadata": {
        "id": "fd08accd"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "import torch\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1771a5b0",
      "metadata": {
        "id": "1771a5b0"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10ce4682",
      "metadata": {
        "id": "10ce4682"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Varun53/openai-roberta-large-AI-detection\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Varun53/openai-roberta-large-AI-detection\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09df2220",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09df2220",
        "outputId": "55262b2d-776d-4a70-e34d-3a6cdecca429"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the text: Artificial Intelligence: The Next Frontier of Human Innovation  Artificial Intelligence (AI) represents one of the most transformative technologies of our time, reshaping industries, enhancing daily life, and challenging our understanding of intelligence and autonomy. Its profound impact stretches across diverse fields, promising unprecedented advancements and posing significant ethical and practical challenges. This essay explores the multifaceted nature of AI, its potential benefits, and the critical considerations it raises for the future.\n"
          ]
        }
      ],
      "source": [
        "text=str(input(\"\"\"Enter the text: \"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7455304f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7455304f",
        "outputId": "899eeb08-9e80-4c57-a4ae-7c22e5647928"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Artificial Intelligence: The Next Frontier of Human Innovation  Artificial Intelligence (AI) represents one of the most transformative technologies of our time, reshaping industries, enhancing daily life, and challenging our understanding of intelligence and autonomy. Its profound impact stretches across diverse fields, promising unprecedented advancements and posing significant ethical and practical challenges. This essay explores the multifaceted nature of AI, its potential benefits, and the critical considerations it raises for the future.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b56774f",
      "metadata": {
        "id": "8b56774f"
      },
      "outputs": [],
      "source": [
        "inputs=tokenizer(text,padding=True, truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d1a0fe6",
      "metadata": {
        "id": "5d1a0fe6"
      },
      "outputs": [],
      "source": [
        "def text_to_sentences(text):\n",
        "    clean_text = text.replace('\\n', ' ')\n",
        "    return re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', clean_text)\n",
        "\n",
        "\n",
        "def chunks_of_2000(text, chunk_size = 2000):\n",
        "    sentences = text_to_sentences(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk + sentence) <= chunk_size:\n",
        "            if len(current_chunk)!=0:\n",
        "                current_chunk += \" \"+sentence\n",
        "            else:\n",
        "                current_chunk += sentence\n",
        "        else:\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = sentence\n",
        "    chunks.append(current_chunk)\n",
        "    return chunks\n",
        "\n",
        "def predict(query):\n",
        "    tokens = tokenizer.encode(query)\n",
        "    all_tokens = len(tokens)\n",
        "    tokens = tokens[:tokenizer.model_max_length - 2]\n",
        "    used_tokens = len(tokens)\n",
        "    tokens = torch.tensor([tokenizer.bos_token_id] + tokens + [tokenizer.eos_token_id]).unsqueeze(0)\n",
        "    mask = torch.ones_like(tokens)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logits = model(**inputs).logits\n",
        "\n",
        "      probs=logits.softmax(dim=-1)\n",
        "\n",
        "\n",
        "    Ai_generated, Human_Generated = probs.detach().cpu().flatten().numpy().tolist()\n",
        "    return Ai_generated\n",
        "\n",
        "def findRealProb(text):\n",
        "    chunksOfText = (chunks_of_2000(text))\n",
        "    results = []\n",
        "    for chunk in chunksOfText:\n",
        "        output = predict(chunk)\n",
        "        results.append([output, len(chunk)])\n",
        "\n",
        "    ans = 0\n",
        "    cnt = 0\n",
        "    for prob, length in results:\n",
        "        cnt += length\n",
        "        ans = ans + prob*length\n",
        "    realProb = ans/cnt\n",
        "    return {\"Ai_Generated\": realProb, \"Human_Generated\": 1-realProb}, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "585bc589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "585bc589",
        "outputId": "ec6d1a29-9fd8-4304-f814-8463584fb171"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'Ai_Generated': 0.6782939434051514, 'Human_Generated': 0.32170605659484863},\n",
              " [[0.6782939434051514, 548]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "findRealProb(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbda490c",
      "metadata": {
        "id": "dbda490c"
      },
      "outputs": [],
      "source": [
        "def get_ai_percentage(text):\n",
        "    inputs=tokenizer(text,padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        ai_probs = logits.softmax(dim=-1)\n",
        "    ai_percentage = ai_probs[0][0] / len(ai_probs)\n",
        "    return ai_percentage * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a74334",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a74334",
        "outputId": "69037ccc-0bc1-4141-eea2-b3c559d3f14e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(67.8294)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "get_ai_percentage(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AI Text Humanizer with Synonym Replacement**\n"
      ],
      "metadata": {
        "id": "4-Q0A5DhpWaQ"
      },
      "id": "4-Q0A5DhpWaQ"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy nltk transformers torch sentencepiece"
      ],
      "metadata": {
        "id": "EsNngtxFpeXC"
      },
      "id": "EsNngtxFpeXC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import wordnet\n",
        "import subprocess\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')  # Download WordNet\n",
        "\n",
        "# Download spaCy model if not already installed\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Check for GPU and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load AI Detector model and tokenizer from Hugging Face (DistilBERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\").to(device)\n",
        "\n",
        "# Load SRDdev Paraphrase model and tokenizer for humanizing text\n",
        "paraphrase_tokenizer = T5Tokenizer.from_pretrained(\"SRDdev/Paraphrase\")\n",
        "paraphrase_model = T5ForConditionalGeneration.from_pretrained(\"SRDdev/Paraphrase\").to(device)\n",
        "\n",
        "# Function to find synonyms using WordNet via NLTK\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "\n",
        "# Replace words with synonyms using spaCy and WordNet\n",
        "def replace_with_synonyms(text):\n",
        "    doc = nlp(text)\n",
        "    processed_text = []\n",
        "    for token in doc:\n",
        "        synonyms = get_synonyms(token.text.lower())\n",
        "        if synonyms and token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}:  # Only replace certain types of words\n",
        "            replacement = synonyms[0]  # Replace with the first synonym\n",
        "            if token.is_title:\n",
        "                replacement = replacement.capitalize()\n",
        "            processed_text.append(replacement)\n",
        "        else:\n",
        "            processed_text.append(token.text)\n",
        "    return \" \".join(processed_text)\n",
        "\n",
        "# AI detection function using DistilBERT\n",
        "def detect_ai_generated(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "    ai_probability = probabilities[0][1].item()  # Probability of being AI-generated\n",
        "    return ai_probability\n",
        "\n",
        "# Humanize the AI-detected text using the SRDdev Paraphrase model\n",
        "def humanize_text(AI_text):\n",
        "    paragraphs = AI_text.split(\"\\n\")\n",
        "    paraphrased_paragraphs = []\n",
        "    for paragraph in paragraphs:\n",
        "        if paragraph.strip():\n",
        "            inputs = paraphrase_tokenizer(paragraph, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "            paraphrased_ids = paraphrase_model.generate(\n",
        "                inputs['input_ids'],\n",
        "                max_length=inputs['input_ids'].shape[-1] + 20,  # Slightly more than the original input length\n",
        "                num_beams=4,\n",
        "                early_stopping=True,\n",
        "                length_penalty=1.0,\n",
        "                no_repeat_ngram_size=3,\n",
        "            )\n",
        "            paraphrased_text = paraphrase_tokenizer.decode(paraphrased_ids[0], skip_special_tokens=True)\n",
        "            paraphrased_paragraphs.append(paraphrased_text)\n",
        "    return \"\\n\\n\".join(paraphrased_paragraphs)\n",
        "\n",
        "# Main function to handle the overall process\n",
        "def main_function(AI_text):\n",
        "    # Replace words with synonyms\n",
        "    text_with_synonyms = replace_with_synonyms(AI_text)\n",
        "\n",
        "    # Detect AI-generated content\n",
        "    ai_probability = detect_ai_generated(text_with_synonyms)\n",
        "\n",
        "    # Humanize AI text\n",
        "    humanized_text = humanize_text(text_with_synonyms)\n",
        "    print(f\"AI-Generated Content: {ai_probability:.2f}%\")\n",
        "    print(\"\\nHumanized Text:\")\n",
        "    print(humanized_text)\n"
      ],
      "metadata": {
        "id": "-lq5vCJvprjy"
      },
      "id": "-lq5vCJvprjy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\"Enter AI-generated text and get a human-written version, with synonyms replaced for more natural output. This space uses models from Hugging Face directly.\"\n",
        "AI_text=str(input(\"\"\"Enter the text: \"\"\"))\n",
        "main_function(AI_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "Bw_dyGrUqeCH",
        "outputId": "d21be564-e0af-448e-bf07-25f084fe590e"
      },
      "id": "Bw_dyGrUqeCH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the text: Artificial Intelligence: The Next Frontier of Human Innovation.  Artificial Intelligence (AI ) is a virtually transformative applied_science of our prison system, changing the way we manufacture, reshape our daily life, challenge our belief in intelligence information and self-sufficiency. Its profound impact stretches across a variety of domains, calls for an unprecedented procession and impersonates significant honorable and practical gainsay. This experiment examines the multifarious nature of AI, its likely benefit, and the vital consideration it has for future.\n",
            "AI-Generated Content: 0.77%\n",
            "\n",
            "Humanized Text:\n",
            "AI is a revolutionary science that alters the organization of our prisons, shifts the way we work, recasts our daily lives, disputes our perception of intelligence data and ego-sufficiency. Its unrefined touch of intelligence and phone call for an unprecedented emanation and represents a new level of ethical and pragmatic uncertainty. This study examines the multifaceted nature of AI, its likely gain, and the critical condition it must meet for its future.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Done'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}